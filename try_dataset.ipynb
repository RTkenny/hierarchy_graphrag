{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rt/data/miniconda3/envs/RAG/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from memorag import Model\n",
    "from functools import partial\n",
    "from functools import partial\n",
    "from transformers.utils import logging\n",
    "from torch.utils.data import DataLoader\n",
    "from longbench_utils import DATASET2CATEGORY, scorer, DATASET2PROMPT, DATASET2MAXNEWTOKENS, makedirs, FileLogger, DefaultDataCollator\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme1/data_rt/hierarchy_graphrag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "gen_model_name_or_path = \"/home/rt/data/model/Qwen/Qwen2.5-7B-Instruct\"\n",
    "load_in_4bit = True\n",
    "enable_flash_attn = False\n",
    "gen_model = Model(gen_model_name_or_path, cache_dir=None, access_token='', load_in_4bit=load_in_4bit, enable_flash_attn=enable_flash_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! The Alternating Direction Method of Multipliers (ADMM) is a popular method for solving convex optimization problems involving a separation of variables. Below is an implementation of the ADMM algorithm in C++ for a simple optimization problem.\n",
      "\n",
      "Let's consider the optimization problem:\n",
      "\n",
      "\\[\n",
      "\\text{minimize} \\quad f(x) + g(z) \\quad \\text{subject to} \\quad Ax + Bz = c\n",
      "\\]\n",
      "\n",
      "Where \\( f \\) and \\( g \\) are convex functions, \\( A \\) is a matrix, \\( B \\) is another matrix, and \\( c \\) is a vector.\n",
      "\n",
      "Here's a basic implementation:\n",
      "\n",
      "```cpp\n",
      "#include <iostream>\n",
      "#include <vector>\n",
      "#include <cmath>\n",
      "\n",
      "// Function to evaluate f(x)\n",
      "double evaluate_f(const double x) {\n",
      "    // Example f(x) = x^2\n",
      "    return x * x;\n",
      "}\n",
      "\n",
      "// Function to evaluate g(z)\n",
      "double evaluate_g(const double z) {\n",
      "    // Example g(z) = z^2\n",
      "    return z * z;\n",
      "}\n",
      "\n",
      "// Function to compute the gradient of f(x)\n",
      "double gradient_f(const double x) {\n",
      "    // Example gradient f'(x) = 2x\n",
      "    return 2 * x;\n",
      "}\n",
      "\n",
      "// Function to compute the gradient of g(z)\n",
      "double gradient_g(const double z) {\n",
      "    // Example gradient g'(z) = 2z\n",
      "    return 2 * z;\n",
      "}\n",
      "\n",
      "// Function to compute the matrix operations Ax and Bz\n",
      "double matrix_operation_Ax(const double x) {\n",
      "    // Example A = [[1]]\n",
      "    return x;\n",
      "}\n",
      "\n",
      "double matrix_operation_Bz(const double z) {\n",
      "    // Example B = [[1]]\n",
      "    return z;\n",
      "}\n",
      "\n",
      "// Solve linear equation Ax + Bz = c\n",
      "std::vector<double> solve_linear_equation(const double c, const double x, const double z) {\n",
      "    // Example A = [[1]], B = [[1]], solve for x and z such that x + z = c\n",
      "    return {c / 2, c / 2};\n",
      "}\n",
      "\n",
      "// ADMM algorithm\n",
      "void admm_algorithm(const double lambda, const double mu, const double theta, int num_iterations, double x, double z) {\n",
      "    double mu_i = mu;\n",
      "    double x_k = x;\n",
      "    double z_k = z;\n",
      "    for (int i = 0; i < num_iterations; ++i) {\n",
      "        // Step 1: Update x\n",
      "        double x_k_new = x_k - mu_i * gradient_f(x_k);\n",
      "        double r_k1 = x_k_new - x_k;\n",
      "        x_k = x_k_new;\n",
      "\n",
      "        // Step 2: Update z\n",
      "        double z_k_new = z_k - mu_i * gradient_g(z_k);\n",
      "        double r_k2 = z_k_new - z_k;\n",
      "        z_k = z_k_new;\n",
      "\n",
      "        // Step 3: Update lambda using dual variable update rule\n",
      "        double lambda_k_new = lambda + mu_i * (matrix_operation_Ax(x_k) + matrix_operation_Bz(z_k) - c);\n",
      "\n",
      "        // Step 4: Update mu\n",
      "        mu_i = theta * mu_i;\n",
      "\n",
      "        // Print output every 100 iterations\n",
      "        if (i % 100 == 0) {\n",
      "            std::cout << \"Iteration: \" << i << \", lambda: \" << lambda_k_new << \", x: \" << x_k << \", z: \" << z_k << std::endl;\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "// Main function\n",
      "int main() {\n",
      "    // Initial values\n",
      "    double x = 1.0;\n",
      "    double z = 1.0;\n",
      "    double c = 1.0;\n",
      "    double lambda = 0.0;\n",
      "    double mu = 1.0;\n",
      "    double theta = 0.1;\n",
      "    int num_iterations = 1000;\n",
      "\n",
      "    // Solve the optimization problem using ADMM\n",
      "    admm_algorithm(lambda, mu, theta, num_iterations, x, z);\n",
      "\n",
      "    // Final values\n",
      "    std::cout << \"Final lambda: \" << lambda << \", x: \" << x_k << \", z: \" << z_k << std::endl;\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Functions `evaluate_f` and `evaluate_g`:** These are the functions \\( f \\) and \\( g \\) that you want to optimize.\n",
      "2. **Functions `gradient_f` and `gradient_g`:** These are the gradients of \\( f \\) and \\( g \\).\n",
      "3. **Function `matrix_operation_Ax` and `matrix_operation_Bz`:** These define the matrix multiplication \\( Ax \\) and \\( Bz \\).\n",
      "4. **Function `solve_linear_equation`:** Solves the linear equation \\( Ax + Bz = c \\).\n",
      "5. **Function `admm_algorithm`:** This function implements the ADMM algorithm.\n",
      "6. **Main function:** This sets up the initial conditions and calls the ADMM algorithm.\n",
      "\n",
      "### Notes:\n",
      "- You need to modify the `evaluate_f`, `evaluate_g`, `gradient_f`, `gradient_g`, `matrix_operation_Ax`, and `matrix_operation_Bz` functions to match your specific problem.\n",
      "- The `theta` parameter controls the step size of the dual variable update.\n",
      "- The number of iterations can be adjusted based on the convergence criteria.\n",
      "\n",
      "### Running the Program:\n",
      "Compile and run the program using a C++ compiler. The output will show the progress of the algorithm at certain iterations.\n",
      "\n",
      "This is a basic implementation and may need to be adjusted and extended for specific applications.\n"
     ]
    }
   ],
   "source": [
    "out = gen_model.generate(\n",
    "    prompts='write a C++ program to implement the admm algorithm',\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=True\n",
    ")\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_longbench(data, indices, tokenizer, max_length=3500, truncate_from_middle=True):\n",
    "    outputs = {'context': [], 'question': [], \"dataset\": [], \"index\": [], \"length\": []}\n",
    "\n",
    "    for input, context, dataset, index in zip(data['input'], data['context'], data['dataset'], indices):\n",
    "        if dataset.endswith(\"_e\"):\n",
    "            dataset = dataset[:-2]\n",
    "\n",
    "        if dataset in ['narrativeqa', 'qasper', 'multifieldqa_en', 'hotpotqa', '2wikimqa', 'musique', 'qmsum']:\n",
    "            question = input\n",
    "        elif dataset == \"gov_report\":\n",
    "            question = \"\"\n",
    "        elif dataset == \"multi_news\":\n",
    "            question = \"\"\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if max_length is not None:\n",
    "            if truncate_from_middle:\n",
    "                try:\n",
    "                    tokenized_context = tokenizer.encode(context, add_special_tokens=False)\n",
    "                except:\n",
    "                    tokenized_context = tokenizer.encode(context)\n",
    "                if len(tokenized_context) > max_length:\n",
    "                    half = int(max_length / 2)\n",
    "                    context = tokenizer.decode(tokenized_context[:half]) + tokenizer.decode(tokenized_context[-half:])\n",
    "            else:\n",
    "                tokenized_context = tokenizer.encode(context)\n",
    "                context = tokenizer.decode(tokenized_context[-max_length:])\n",
    "\n",
    "        length = len(tokenizer.encode(context))\n",
    "\n",
    "        outputs[\"context\"].append(context)\n",
    "        outputs[\"question\"].append(question)\n",
    "        outputs[\"dataset\"].append(dataset)\n",
    "        outputs[\"index\"].append(index)\n",
    "        outputs[\"length\"].append(length)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results/longbench/\"\n",
    "\n",
    "dataset_names = ['narrativeqa', 'qasper', 'multifieldqa_en', 'hotpotqa', '2wikimqa', 'musique'] # ['narrativeqa', 'qasper', 'hotpotqa']\n",
    "# raw_dataset = datasets.load_dataset(\"json\", data_files=f'/home/rt/data/MemoRAG/THUDM/LongBench/data/{dataset_names[0]}.jsonl', split=\"train\")\n",
    "raw_dataset = datasets.load_dataset(\"json\", data_files='../dataset/TommyChien/MemoRAG-data/longbench.json', split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100000\n",
    "truncate_from_middle = True\n",
    "\n",
    "process_fn = partial(\n",
    "            process_longbench, \n",
    "            tokenizer=gen_model.tokenizer,\n",
    "            max_length=max_length,\n",
    "            truncate_from_middle=truncate_from_middle\n",
    "        )\n",
    "\n",
    "dataset = raw_dataset.map(process_fn, batched=True, num_proc=32, with_indices=True, remove_columns=raw_dataset.column_names)\n",
    "groupby_dataset = dataset.to_pandas().groupby(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   2%|▏         | 4/200 [00:12<08:06,  2.48s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Generating: 100%|██████████| 200/200 [15:42<00:00,  4.71s/it]\n",
      "Generating: 100%|██████████| 200/200 [03:03<00:00,  1.09it/s]\n",
      "Generating: 100%|██████████| 150/150 [03:30<00:00,  1.41s/it]\n",
      "Generating: 100%|██████████| 200/200 [05:53<00:00,  1.77s/it]\n",
      "Generating: 100%|██████████| 200/200 [03:26<00:00,  1.03s/it]\n",
      "Generating: 100%|██████████| 200/200 [07:29<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "metrics = {}\n",
    "result_dir = ''\n",
    "result_dir = os.path.join(output_dir, result_dir)\n",
    "\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    logger.info(f\"Evaluating {dataset_name} ({i + 1} / {len(dataset_names)})...\")\n",
    "\n",
    "    result_path = os.path.join(result_dir, f\"{dataset_name}.json\")\n",
    "    \n",
    "    dataset = datasets.Dataset.from_pandas(groupby_dataset.get_group(dataset_name), preserve_index=False)\n",
    "\n",
    "    data_collator = DefaultDataCollator(padding_side=\"left\")\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=1, \n",
    "        collate_fn=data_collator,\n",
    "        # only pin memory when no gpu\n",
    "    )\n",
    "\n",
    "    indices = []\n",
    "    preds = []\n",
    "    memory_results = []\n",
    "    _prompt = DATASET2PROMPT[dataset_name]\n",
    "    task_max_new_token=DATASET2MAXNEWTOKENS[dataset_name]\n",
    "    \n",
    "    for i, x in enumerate(tqdm(dataloader, desc=\"Generating\")):\n",
    "        x.pop(\"dataset\")\n",
    "        index = x.pop(\"index\")[0]\n",
    "        \n",
    "        # if \"QA\" in DATASET2CATEGORY[dataset_name]:\n",
    "        #     output = [pipe(x[\"context\"][0], x[\"question\"][0], prompt_template=_prompt, task_type=\"rag\", max_new_tokens=task_max_new_token, reset_each_call=True, use_memory_answer=True)]\n",
    "        # else:\n",
    "        #     output = [pipe(x[\"context\"][0], x[\"question\"][0], prompt_template=_prompt, task_type=\"summarize\", max_new_tokens=task_max_new_token, reset_each_call=True, use_memory_answer=True)]\n",
    "\n",
    "        # generate output\n",
    "        prompt = _prompt.format(context=x[\"context\"][0], input=x[\"question\"][0])\n",
    "        output = gen_model.generate(prompts=prompt, max_new_tokens=task_max_new_token, do_sample=True)\n",
    "\n",
    "        # print(output)\n",
    "\n",
    "        index = index.tolist()\n",
    "        preds.extend(output)\n",
    "        if isinstance(index, list):\n",
    "            indices.extend(index)\n",
    "        else:\n",
    "            # single process\n",
    "            indices.append(index)\n",
    "\n",
    "        raw_dataset_subset = raw_dataset[indices]\n",
    "        answers = raw_dataset_subset[\"answers\"]\n",
    "        lengths = raw_dataset_subset[\"length\"]\n",
    "        all_classes = []\n",
    "        score = scorer(dataset_name, preds, answers, all_classes)        \n",
    "        \n",
    "        logger.info(f\"{dataset_name}: {score}\")\n",
    "        metrics[dataset_name] = score\n",
    "\n",
    "        with open(makedirs(result_path), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(score, ensure_ascii=False) + \"\\n\")\n",
    "            for index, pred in zip(indices, preds):\n",
    "                sample = raw_dataset[index]\n",
    "                del sample[\"context\"]\n",
    "                sample[\"pred\"] = pred\n",
    "                f.write(json.dumps(sample, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
