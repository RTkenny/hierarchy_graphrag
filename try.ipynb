{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 06:06:44,354 - Loading faiss with AVX2 support.\n",
      "2024-10-10 06:06:44,405 - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from threading import Lock\n",
    "from multiprocessing import Manager\n",
    "import tiktoken\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from raptor.utils import split_text\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader, TextLoader, PyPDFLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T22:06:45.439095500Z",
     "start_time": "2024-10-09T22:06:33.032063600Z"
    }
   },
   "id": "8e1ea9b32c93846",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 06:07:23,167 - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text1s): 22; len(text2s): 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 06:07:24,140 - Collection cinderella is not created.\n"
     ]
    }
   ],
   "source": [
    "# set the  key in the env variable\n",
    "\n",
    "pdf_path = 'demo/RS_zishi.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pdf_content = loader.load()\n",
    "text = ' '.join([page.page_content for page in pdf_content])\n",
    "text1 = text[:10000]\n",
    "text2 = text[10000:20000]\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "text1s = split_text(text1, tokenizer=tokenizer, max_tokens=100)\n",
    "text2s = split_text(text2, tokenizer=tokenizer, max_tokens=100)\n",
    "print(f'len(text1s): {len(text1s)}; len(text2s): {len(text2s)}')\n",
    "\n",
    "client = chromadb.PersistentClient(path='./db2')\n",
    "default_ef = embedding_functions.OpenAIEmbeddingFunction(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "# default_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name='D:/python_data/model/BAAI/bge-large-zh-v1.5')\n",
    "collection = client.get_or_create_collection(name=f'cinderella', embedding_function=default_ef)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T22:07:24.156370400Z",
     "start_time": "2024-10-09T22:07:21.507334900Z"
    }
   },
   "id": "5d1c8e46109d72d3",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "collection.add(documents=text1s, ids=[f'ids{i}' for i in range(len(text1s))], metadatas=[{'type': 'children'}]*len(text1s))\n",
    "collection.add(documents=text2s, ids=[f'ids{len(text1s)+i}' for i in range(len(text2s))], metadatas=[{'type': 'parents'}]*len(text2s))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82acb5a2241801e3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 06:08:08,457 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'ids': [['ids0', 'ids1', 'ids10', 'ids6', 'ids20']],\n 'distances': [[0.416904749565347,\n   0.4686589294122239,\n   0.5147696403390115,\n   0.5175526835741324,\n   0.5422456518640746]],\n 'metadatas': [[{'type': 'children'},\n   {'type': 'children'},\n   {'type': 'children'},\n   {'type': 'children'},\n   {'type': 'children'}]],\n 'embeddings': None,\n 'documents': [['Sample-Efficient Clustering and Conquer Procedures for Parallel Large-Scale Ranking and Selection Zishi Zhang Wuhan Institute of Aritificial Intelligence, Guanghua School of Management, Peking University, Beijing, China; Xiangjiang Laboratory, Changsha, China, zishizhang@stu pku edu cn Yijie Peng Wuhan Institute of Aritificial Intelligence, Guanghua School of Management, Peking University, Beijing, China;',\n   'Xiangjiang Laboratory, Changsha, China, pengyijie@gsm pku edu cn We propose novel “clustering and conquer” procedures for the parallel large-scale ranking and selection (R&S) problem, which leverage correlation information for clustering to break the bottleneck of sample efficiency  In parallel computing environments, correlation-based clustering can achieve an O(p) sample com- plexity reduction rate, which is the optimal reduction rate theoretically attainable  Our proposed framework',\n   'alternatives demonstrates that the screening- freeversion of P3C requires only 2% of the sample Zhang and Peng: Parallel Correlation-Based Clustering And Conquer3 size compared to traditional stage-wise procedure (Rinott 1978) and even outperforms KT, one of the most sample-efficient screening-based procedures, by using only 60% of the sample size  To illustrate the motivation of P3C, we provide the following examples',\n   'overcoming the limitations of the all pairwise comparison paradigm  For instance, Zhong and Hong (2022) propose a Knockout Tournament (KT) procedure to restrict comparisons to matches involving only two alternatives  Pei et al  (2022) compare each alternative against a common standard to avoid exhaustive pairwise comparisons  On the contrary, stage-wise methods, which predetermine the number of replications, are more naturally parallelizable  Nonetheless, a notable',\n   'framework that also accommodates the use of fully-sequential procedures with screening, such as GSP (Ni et al  2017) and KT, significantly outperforming their counterparts without using P3C A consensus in large-scale R&S literature is that there may exist a large number of acceptably good alternatives with means lying within a δrange around the best one (Ni et al  2017, Pei et al 2022)']],\n 'uris': None,\n 'data': None}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = collection.query(query_texts='zishizhang', n_results=5, where={'type': 'children'})\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T22:08:08.539501Z",
     "start_time": "2024-10-09T22:08:07.379546Z"
    }
   },
   "id": "662191e1b17f677f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Collection(name=cinderella)]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_collections()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T22:15:37.640880100Z",
     "start_time": "2024-10-09T22:15:37.589986Z"
    }
   },
   "id": "61ad4944ebaf7ac7",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'ids': ['ids0',\n  'ids1',\n  'ids10',\n  'ids11',\n  'ids12',\n  'ids13',\n  'ids14',\n  'ids15',\n  'ids16',\n  'ids17',\n  'ids18',\n  'ids19',\n  'ids2',\n  'ids20',\n  'ids21',\n  'ids22',\n  'ids23',\n  'ids24',\n  'ids25',\n  'ids26',\n  'ids27',\n  'ids28',\n  'ids29',\n  'ids3',\n  'ids30',\n  'ids31',\n  'ids32',\n  'ids33',\n  'ids34',\n  'ids35',\n  'ids36',\n  'ids37',\n  'ids38',\n  'ids39',\n  'ids4',\n  'ids40',\n  'ids41',\n  'ids42',\n  'ids43',\n  'ids44',\n  'ids45',\n  'ids46',\n  'ids5',\n  'ids6',\n  'ids7',\n  'ids8',\n  'ids9'],\n 'embeddings': None,\n 'metadatas': [{'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'children'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'children'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'}],\n 'documents': ['Sample-Efficient Clustering and Conquer Procedures for Parallel Large-Scale Ranking and Selection Zishi Zhang Wuhan Institute of Aritificial Intelligence, Guanghua School of Management, Peking University, Beijing, China; Xiangjiang Laboratory, Changsha, China, zishizhang@stu pku edu cn Yijie Peng Wuhan Institute of Aritificial Intelligence, Guanghua School of Management, Peking University, Beijing, China;',\n  'Xiangjiang Laboratory, Changsha, China, pengyijie@gsm pku edu cn We propose novel “clustering and conquer” procedures for the parallel large-scale ranking and selection (R&S) problem, which leverage correlation information for clustering to break the bottleneck of sample efficiency  In parallel computing environments, correlation-based clustering can achieve an O(p) sample com- plexity reduction rate, which is the optimal reduction rate theoretically attainable  Our proposed framework',\n  'alternatives demonstrates that the screening- freeversion of P3C requires only 2% of the sample Zhang and Peng: Parallel Correlation-Based Clustering And Conquer3 size compared to traditional stage-wise procedure (Rinott 1978) and even outperforms KT, one of the most sample-efficient screening-based procedures, by using only 60% of the sample size  To illustrate the motivation of P3C, we provide the following examples',\n  '•Drug discovery : As depicted in Figure 1(a), the process of developing new drugs typ- ically starts with a base molecule, followed by creating multiple variants through atom sub- stitution (Negoescu et al  2011)  The Free-Wilson model (Free and Wilson 1964) describes a drug’s effect as the summation of the effects of each substitution atom: the value of the drug =',\n  'value of the base molecule +value of atom X +···  This can be viewed as a latent factor model, where the behavior of each atom is a random variable (latent factor)  In large-scale scenarios, numerous drugs influenced by shared latent factors may exhibit significant correlation and show a clear clustering phenomenon (see Figure 4(b)) •Neural Architecture Search (NAS) : NAS, a key challenge in deep learning, aims to search for the best-performing neural architecture design',\n  ' As depicted in Figure 1(b), the large search space is composed of combinations of multiple decisions, including the number of layers, the type of activation functions, the type of operations and so on  Nonetheless, evaluating and ranking architectures on test datasets is computationally intensive  To reduce the problem dimension, many state-of-the-art algorithms (Guo et al  2020, Mellor et al  2021) leverage the shared information among structurally similar architectures',\n  'Motivated by the practical examples, the intuition behind P3C is to reduce the complexity of the problem by selecting a representative from a cluster of highly correlated alternatives  Note that, in this study, the correlation between alternatives serves as a general measure of inherent similarity, arising from various shared latent factors, not limited to common random numbers (CRN)  P3C can be seen as a variant of the “divide and conquer” strategy in parallel R&S (Ni et al',\n  ' 2017, Zhong and Hong 2022)  It clusters alternatives based on correlations, assigns each cluster to a single processor, and then selects a representative local best from each cluster for final comparison A related study by Li et al  (2022) uses Gaussian Mixture Models (GMM), a clustering algorithm Zhang and Peng: Parallel Correlation-Based Clustering And Conquer4 (a)  (b) Figure 1 (a) Drug Discovery',\n  ' (b) Neural Architecture Search based on mean performance, to extract shared information in non-parallel settings  However, in parallel R&S, clustering alternatives with similar mean performance on the same processor can be detrimental due to the increased challenge in differentiating them, while distributing them to different processors can also be harmful due to the communication overhead when extracting shared information  In contrast, P3C uses correlation-based clustering, where alternatives in the same cluster have high correlations but may differ significantly in mean performance',\n  ' We prove that clustering together highly correlated alternatives can accelerate the R&S procedure  This advantage is uncovered by establishing a theoretical underpinning of the interaction between mean and correlation: adding correlation can yield a “separation” effect, which probabilistically amplifies good alternatives while suppressing the bad ones  Another intuitive explanation is that clustering highly correlated alternatives together effectively cancels out stochastic fluctuations in the same direction  The stronger the correlation, the more pronounced this effect becomes  These concepts',\n  'parallel the CRN technique, which introduces positive correlation artificially to expedite pairwise comparisons R&S problems can be formulated as either fixed-precision or fixed-budget paradigm (Hunter and Nelson 2017), and most of the prominent large-scale methods adopt the former formulation However, in certain large-scale applications such as NAS, achieving a high confidence level is often infeasible due to the limited availability of public datasets',\n  ' Our screening-free version of P3C, Zhang and Peng: Parallel Correlation-Based Clustering And Conquer5 referred to as P3C-GBA, uses a sequential generalized budget allocation (GBA) algorithm equipped with stopping rules, similar to Branke et al  (2007) and Eckman and Henderson (2022)  P3C-GBA is applicable to both formulations, depending on the stopping rule employed  P3C is a flexible',\n  'is versatile, allowing for seamless integration of various prevalent R&S methods under both fixed-budget and fixed-precision paradigms  It can achieve improvements without the necessity of highly accurate corre- lation estimation and precise clustering  In large-scale AI applications such as neural architecture search, a screening-free version of our procedure surprisingly surpasses fully-sequential benchmarks in terms of sample efficiency  This suggests that leveraging valuable structural information, such as correlation, is a viable path',\n  'framework that also accommodates the use of fully-sequential procedures with screening, such as GSP (Ni et al  2017) and KT, significantly outperforming their counterparts without using P3C A consensus in large-scale R&S literature is that there may exist a large number of acceptably good alternatives with means lying within a δrange around the best one (Ni et al  2017, Pei et al 2022)',\n  ' In such “low-confidence scenarios” (Peng et al  2017) characterized by similar means and finite sample sizes, we find that correlation information becomes a crucial factor in evaluating alternatives  Relying solely on mean information would neglect alterna',\n  'tives with a much higher probability of being the best  Therefore, we select the alternative that maximizes the individual probability of correct selection (PCS), termed probabilistic optimal selection (P-OS), instead of simply choosing the largest sample mean  Similar probability-based selection policies are often adopted in the Bayesian framework (Peng et al  2016, Russo 2020, Kim et al  2022), and the integration of similarity information into selection policies is explored in Zhou et al',\n  ' (2023)  Addi- tionally, we prove that P-OS can identify an alternative within each cluster that not only has a good mean performance but also effectively represents the cluster’s collective information  Specifi- cally, a representative alternative refers to a “central” one which exhibits strong correlations with other alternatives within the same cluster  This viewpoint is the foundation of Principal Compo- nent Analysis (PCA), a successful dimension reduction method in machine learning (Al-Kandari',\n  'and Jolliffe 2001, Shlens 2014)  With P-OS, P3C is analogous to PCA-based variable selection techniques (Jolliffe 1972, Enki et al  2013) The main contributions of this paper can be summarized as follows •We propose P3C procedures for solving large-scale R&S problems  We establish a theoret- ical underpinning of the interaction between mean and correlation information and prove that',\n  'correlation-based clustering can achieve rate-optimal sample complexity reduction  Zhang and Peng: Parallel Correlation-Based Clustering And Conquer6 •We introduce a new selection policy, named P-OS, tailored for large-scale scenarios, along with a corresponding GBA sampling policy •We propose a parallelizable few-shot alternative clustering algorithm, denoted as AC+  This algorithm eliminates the need to estimate the entire correlation matrix and only requires a small submatrix, effectively addressing the challenges of large-scale clustering',\n  ' We introduce the proba- bility of correct clustering (PCC) to quantify the clustering quality 2  Problem Formulation LetP={1,2, ,p}denote the index set for all palternatives  We adopt a Frequentist frame- work, and the output of alternative i∈ Pis a random variable Xi  We assume that the population distribution of the random vector ( X1,X2,',\n  ',X p) is multivariate normal N(µ,Σp×p), where µ= (µ1,···,µp) is the mean vector and Σ p×pis the covariance matrix  Let xijdenote the jth sim- ulation observation of alternative i  The observation vectors ( x1j,x2j,···,xpj)∼N(µ,Σp×p) are independently and identically distributed  We assume',\n  'cov(xim,xjn) =\\uf8f1 \\uf8f4\\uf8f4\\uf8f2 \\uf8f4\\uf8f4\\uf8f30 m̸=n cov(Xi,Xj)m=n, where cov(X,Y) represents the covariance between random variables XandY  Let Nidenote the individual sample size allocated to alternative iand ¯xibe the sample average, then cov(¯xi,¯xj)= cov(Xi,Xj) max( Ni,Nj)',\n  ' The objective of R&S is to identify the true best alternative [1]= arg max i∈Pµi The correlation within the distribution arises from the influence of common latent factors, reflecting the similarity between alternatives  Suppose that the palternatives come from knon- overlapping clusters: G1, ,Gk(k≤p)with cardinality |Gj|=pj(Pk j=1pj=p) Gis a mapping from Pto{1,2,',\n  'to bypassing the traditional need for screening via pairwise comparison—a step previously deemed essential for high sample efficiency but problematic for parallelization  Additionally, we propose a parallel few-shot clustering algorithm tailored for large-scale problems Key words : simulation, ranking and selection, parallel computing 1  Introduction Ranking and selection (R&S) aims to identify the best design from a finite set of alternatives, through conducting simulation and learning about their performances  In recent years, the large-',\n  ',k}, where G(i)=jif alternative ibelongs to Gj  Π = ( G(1),G(2), ,G (p))∈Rp represents the true cluster partition of palternatives (unknown) and we need sampling to recover it  Alternatives within the same cluster are highly correlated (although their mean performances may vary considerably), whereas alternatives from different clusters are less correlated  We assume',\n  'that the Pearson correlation coefficient between any two alternatives belonging to the same cluster Zhang and Peng: Parallel Correlation-Based Clustering And Conquer7 exceeds that between any two alternatives from different clusters  Notably, the independent case is also included by setting p=k Assumption 1  rij> rmnfor all i,j,m,n ∈ Psuch that G(i) =G(j)andG(m)̸=G(n), where rij',\n  'denotes the correlation coefficient between XiandXjand0< rij<1 3  The Framework of Parallel Correlation-Based Clustering and Conquer Since the alternatives in the same cluster reflect common latent factors and attributes, one may select a representative alternative from each cluster to reduce complexity  For example, if the outputs of two alternatives satisfy X1=X2+ϵwhere ϵrepresents white noise, we only need to simulate one of them to gain knowledge about both alternatives',\n  ' The development of P3C procedures is inspired by this intuition 3 1  Clustering and Conquer As outlined in Procedure 1 and Figure 2, P3C starts with an initialization Stage 0  Then, in Stage 1, we continue sampling and then cluster alternatives based on correlations  The sample size can be determined in advance based on the required clustering accuracy  Additionally, the novel few-shot clustering algorithm AC+, detailed in Chapter 5, enables the effective parallelization of',\n  'both Stage 0 and Stage 1 in large-scale problems  Upon completion of Stage 1, the alternatives of the same cluster are sent to a single processor  During Stage 2, we continue sampling and then select the local best within each cluster  This stage exclusively involves the comparison within the cluster, without any information exchange between clusters  The sampling strategy here is flexible, accommodating various existing fully-sequential methods like KT and GSP, which typically involve screening, i e',\n  ', discarding clearly inferior alternatives early on  We also provide a screening-free version that employs a sequential GBA sampling algorithm with stopping rules, referred to as P3C- GBA (see Chapter 6)  In Stage 3, we simulate and compare these kselected local bests to determine the final winner  Regarding statistical validity, if each cluster achieves a precision of 1 −α1in Stage',\n  '2, and if Stage 3 achieves 1 −α2, the overall statistical guarantee will be 1 −(α1+α2)  In practice, we set α1to be much larger than α2since pis much larger than k  Zhang and Peng: Parallel Correlation-Based Clustering And Conquer8 Procedure 1 Parallel Correlation-based Clustering and Conquer (P3C) Stage 0 (Initialization)',\n  ' Simulate N0times for each alternative and estimate the correlation matrix Stage 1 (Alternative clustering)  Continue sampling  Perform clustering algorithm AC+to group all palternatives into kcorrelated clusters  After that, assign the alternatives of the same cluster to one processor Stage 2 (Selecting the representative local best)  Continue sampling (using KT, GSP or GBA) and calculate the P-OS within each cluster Stage 3 (Final comparison)',\n  ' R&S on the selected klocal P-OSs and find the global optimal alternative Figure 2 Parallel correlation-based clustering and conquer P3C can be viewed as a variant of the “divide and conquer” strategy (Ni et al  2017, Zhong and Hong 2022)  The first departure is that P3C performs correlation-based clustering and then systematically assigns alternatives from the same cluster to a single processor, rather than ran- domly assigning them',\n  ' We prove that correlation-based clustering can bring rate-optimal sample complexity reduction  The second departure is the adoption of the aforementioned sequential GBA algorithm (optional)  It can adapt to both fixed-precision and fixed-budget formulations  Moreover, it leverages correlation information and exhibits high sample efficiency in large-scale experiments  Zhang and Peng: Parallel Correlation-Based Clustering And Conquer9 3 2  Probabilistic Optimal Selection Policy',\n  'scale R&S problem, especially in the context of parallel computing environments, has emerged as an important research topic (Luo et al  2015, Ni et al  2017, Zhong and Hong 2022)  Here, 1arXiv:2402 02196v2  [stat ME]  13 Feb 2024 Zhang and Peng: Parallel Correlation-Based Clustering And Conquer2',\n  'The third departure from the original “divide and conquer” is adopting a new selection policy  The selection policy τis defined as a mapping from the information set to P(Peng et al  2016)  In traditional R&S procedures, the selection policy is specified as τm= arg maxi∈P¯xi(or maximizing the posterior mean in Bayesian framework), and correspondingly, the traditional PCS is defined as PCS trad≜P\\x00',\n  '¯x[1]>¯xj,j̸= [1]\\x01  To incorporate correlation information into the selection criterion, we borrow an idea from Bayesian R&S, which selects the one maximizing the posterior PCS rather than simply maximizing the posterior mean (Peng et al  2016, Russo 2020, Kim et al  2022)  First,',\n  'for any τ∈ P, we define the individual PCS of τas PCS (τ)≜P(¯xτ>¯xj,j̸=τ)  This is the natural generalization of traditional PCS as PCS trad= PCS ([1])  According to Hong et al  (2021), the statistical meaning of PCS (τ)is the probability of rejecting the null hypothesis that “ τisnotthe true best”, serving as a metric to evaluate the performance of τ',\n  ' Then the P-OS of Pis defined as the alternative maximizing PCS (τ): τ∗≜arg max τ∈PPCS(τ)  (1) This definition can be readily adapted to any cluster Gj⊆ P,j= 1,···,k  In practice, PCS (τ)and τ∗are calculated by plugging in parameters estimated with samples collected so far  Naturally,',\n  'the P-OS policy corresponds to a new statistical guarantee: PCS( τ∗), referred to as the the most optimistic probability of correct selection (moPCS)  The moPCS can be used as a relaxed probability guarantee because moPCS ≥PCS trad  In our large-scale AI experiments, moPCS can even be several tens of times higher than PCS trad The benefits and properties of the P-OS policy, extensively discussed in Chapter 4, can be sum- marized as follows',\n  ' First, the P-OS will converge to [1] as the sample size approaches infinity Second, with finite samples, the P-OS policy is proved to select a desirable representative local best within each cluster that not only has a large mean but also exhibits high correlations with other cluster members  This brings us back to the intuition behind P3C, i e , to reduce the problem Zhang and Peng: Parallel Correlation-Based Clustering And Conquer10 complexity by selecting a representative from each cluster',\n  ' Third, while the analysis and method- ologies in this paper remain valid for the traditional selection policy τmand performance metric PCS trad(by letting τ∗= [1]), the adoption of P-OS and moPCS is particularly advan',\n  '“large-scale” refers to a large number of alternatives, denoted by p  Most of the prominent existing large-scale R&S procedures are fully-sequential in nature, which are usually more sample-efficient due to the inclusion of screening steps based on pairwise comparisons  However, they encounter considerable hurdles in parallel implementation due to frequent information communication and synchronization issues (Luo et al  2015, Ni et al  2017)  Recent advancements have focused on',\n  'overcoming the limitations of the all pairwise comparison paradigm  For instance, Zhong and Hong (2022) propose a Knockout Tournament (KT) procedure to restrict comparisons to matches involving only two alternatives  Pei et al  (2022) compare each alternative against a common standard to avoid exhaustive pairwise comparisons  On the contrary, stage-wise methods, which predetermine the number of replications, are more naturally parallelizable  Nonetheless, a notable',\n  'drawback of these methods is their typically high requirement for sample sizes To break this dilemma, we propose parallel correlation-based clustering and conquer (P3C) pro- cedures, which leverage correlation information among alternatives for clustering to enhance sample efficiency  Existing R&S methods typically assume independence among alternatives and discard shared structural information (Eckman and Henderson 2022), with exceptions such as Fu et al (2007), Qu et al  (2015), Li et al',\n  ' (2022), Zhou et al  (2023)  However, none of these existing meth- ods that utilize shared information is tailored for large-scale problems, and they lack a rigorous mathematical analysis to elucidate and quantify the benefits derived from integrating similarity information, despite showing promising experimental results  Zhong and Hong (2022) point out that it is important to understand the growth rate of the required total sample size as pincreases',\n  'Along this line, we prove that correlation-based clustering techniques in P3C can achieve an O(p) reduction in sample complexity, which is theoretically the optimal reduction rate Screening via frequent pairwise comparison is often considered to be the cornerstone for achieving high sample efficiency in R&S (Hunter and Nelson 2017), despite its difficulty in parallelization  In this paper, valuable correlation information offers a new promising avenue: an experiment with 105'],\n 'uris': None,\n 'data': None}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.get()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T22:08:32.801492400Z",
     "start_time": "2024-10-09T22:08:32.781347500Z"
    }
   },
   "id": "cab6777b0620d165",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from raptor import RetrievalAugmentation\n",
    "RA = RetrievalAugmentation()\n",
    "RA.add_documents(text1)\n",
    "question = \"Why the P3C have better performance than the EA ?\"\n",
    "context, layer_info = RA.retrieve(question=question, retrieve_mode='bottom_up')\n",
    "print(layer_info)\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "print(\"Answer: \", answer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b6d203a71e68b92"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'ids': ['ids0',\n  'ids1',\n  'ids10',\n  'ids11',\n  'ids12',\n  'ids13',\n  'ids14',\n  'ids15',\n  'ids16',\n  'ids17',\n  'ids18',\n  'ids19',\n  'ids2',\n  'ids20',\n  'ids21',\n  'ids22',\n  'ids23',\n  'ids24',\n  'ids25',\n  'ids26',\n  'ids27',\n  'ids28',\n  'ids29',\n  'ids3',\n  'ids30',\n  'ids31',\n  'ids32',\n  'ids33',\n  'ids34',\n  'ids35',\n  'ids36',\n  'ids37',\n  'ids38',\n  'ids39',\n  'ids4',\n  'ids40',\n  'ids41',\n  'ids42',\n  'ids43',\n  'ids44',\n  'ids45',\n  'ids46',\n  'ids5',\n  'ids6',\n  'ids7',\n  'ids8',\n  'ids9'],\n 'embeddings': None,\n 'metadatas': [{'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'children'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'children'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'parents'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'},\n  {'type': 'children'}],\n 'documents': ['Sample-Efficient Clustering and Conquer Procedures for Parallel Large-Scale Ranking and Selection Zishi Zhang Wuhan Institute of Aritificial Intelligence, Guanghua School of Management, Peking University, Beijing, China; Xiangjiang Laboratory, Changsha, China, zishizhang@stu pku edu cn Yijie Peng Wuhan Institute of Aritificial Intelligence, Guanghua School of Management, Peking University, Beijing, China;',\n  'Xiangjiang Laboratory, Changsha, China, pengyijie@gsm pku edu cn We propose novel “clustering and conquer” procedures for the parallel large-scale ranking and selection (R&S) problem, which leverage correlation information for clustering to break the bottleneck of sample efficiency  In parallel computing environments, correlation-based clustering can achieve an O(p) sample com- plexity reduction rate, which is the optimal reduction rate theoretically attainable  Our proposed framework',\n  'alternatives demonstrates that the screening- freeversion of P3C requires only 2% of the sample Zhang and Peng: Parallel Correlation-Based Clustering And Conquer3 size compared to traditional stage-wise procedure (Rinott 1978) and even outperforms KT, one of the most sample-efficient screening-based procedures, by using only 60% of the sample size  To illustrate the motivation of P3C, we provide the following examples',\n  '•Drug discovery : As depicted in Figure 1(a), the process of developing new drugs typ- ically starts with a base molecule, followed by creating multiple variants through atom sub- stitution (Negoescu et al  2011)  The Free-Wilson model (Free and Wilson 1964) describes a drug’s effect as the summation of the effects of each substitution atom: the value of the drug =',\n  'value of the base molecule +value of atom X +···  This can be viewed as a latent factor model, where the behavior of each atom is a random variable (latent factor)  In large-scale scenarios, numerous drugs influenced by shared latent factors may exhibit significant correlation and show a clear clustering phenomenon (see Figure 4(b)) •Neural Architecture Search (NAS) : NAS, a key challenge in deep learning, aims to search for the best-performing neural architecture design',\n  ' As depicted in Figure 1(b), the large search space is composed of combinations of multiple decisions, including the number of layers, the type of activation functions, the type of operations and so on  Nonetheless, evaluating and ranking architectures on test datasets is computationally intensive  To reduce the problem dimension, many state-of-the-art algorithms (Guo et al  2020, Mellor et al  2021) leverage the shared information among structurally similar architectures',\n  'Motivated by the practical examples, the intuition behind P3C is to reduce the complexity of the problem by selecting a representative from a cluster of highly correlated alternatives  Note that, in this study, the correlation between alternatives serves as a general measure of inherent similarity, arising from various shared latent factors, not limited to common random numbers (CRN)  P3C can be seen as a variant of the “divide and conquer” strategy in parallel R&S (Ni et al',\n  ' 2017, Zhong and Hong 2022)  It clusters alternatives based on correlations, assigns each cluster to a single processor, and then selects a representative local best from each cluster for final comparison A related study by Li et al  (2022) uses Gaussian Mixture Models (GMM), a clustering algorithm Zhang and Peng: Parallel Correlation-Based Clustering And Conquer4 (a)  (b) Figure 1 (a) Drug Discovery',\n  ' (b) Neural Architecture Search based on mean performance, to extract shared information in non-parallel settings  However, in parallel R&S, clustering alternatives with similar mean performance on the same processor can be detrimental due to the increased challenge in differentiating them, while distributing them to different processors can also be harmful due to the communication overhead when extracting shared information  In contrast, P3C uses correlation-based clustering, where alternatives in the same cluster have high correlations but may differ significantly in mean performance',\n  ' We prove that clustering together highly correlated alternatives can accelerate the R&S procedure  This advantage is uncovered by establishing a theoretical underpinning of the interaction between mean and correlation: adding correlation can yield a “separation” effect, which probabilistically amplifies good alternatives while suppressing the bad ones  Another intuitive explanation is that clustering highly correlated alternatives together effectively cancels out stochastic fluctuations in the same direction  The stronger the correlation, the more pronounced this effect becomes  These concepts',\n  'parallel the CRN technique, which introduces positive correlation artificially to expedite pairwise comparisons R&S problems can be formulated as either fixed-precision or fixed-budget paradigm (Hunter and Nelson 2017), and most of the prominent large-scale methods adopt the former formulation However, in certain large-scale applications such as NAS, achieving a high confidence level is often infeasible due to the limited availability of public datasets',\n  ' Our screening-free version of P3C, Zhang and Peng: Parallel Correlation-Based Clustering And Conquer5 referred to as P3C-GBA, uses a sequential generalized budget allocation (GBA) algorithm equipped with stopping rules, similar to Branke et al  (2007) and Eckman and Henderson (2022)  P3C-GBA is applicable to both formulations, depending on the stopping rule employed  P3C is a flexible',\n  'is versatile, allowing for seamless integration of various prevalent R&S methods under both fixed-budget and fixed-precision paradigms  It can achieve improvements without the necessity of highly accurate corre- lation estimation and precise clustering  In large-scale AI applications such as neural architecture search, a screening-free version of our procedure surprisingly surpasses fully-sequential benchmarks in terms of sample efficiency  This suggests that leveraging valuable structural information, such as correlation, is a viable path',\n  'framework that also accommodates the use of fully-sequential procedures with screening, such as GSP (Ni et al  2017) and KT, significantly outperforming their counterparts without using P3C A consensus in large-scale R&S literature is that there may exist a large number of acceptably good alternatives with means lying within a δrange around the best one (Ni et al  2017, Pei et al 2022)',\n  ' In such “low-confidence scenarios” (Peng et al  2017) characterized by similar means and finite sample sizes, we find that correlation information becomes a crucial factor in evaluating alternatives  Relying solely on mean information would neglect alterna',\n  'tives with a much higher probability of being the best  Therefore, we select the alternative that maximizes the individual probability of correct selection (PCS), termed probabilistic optimal selection (P-OS), instead of simply choosing the largest sample mean  Similar probability-based selection policies are often adopted in the Bayesian framework (Peng et al  2016, Russo 2020, Kim et al  2022), and the integration of similarity information into selection policies is explored in Zhou et al',\n  ' (2023)  Addi- tionally, we prove that P-OS can identify an alternative within each cluster that not only has a good mean performance but also effectively represents the cluster’s collective information  Specifi- cally, a representative alternative refers to a “central” one which exhibits strong correlations with other alternatives within the same cluster  This viewpoint is the foundation of Principal Compo- nent Analysis (PCA), a successful dimension reduction method in machine learning (Al-Kandari',\n  'and Jolliffe 2001, Shlens 2014)  With P-OS, P3C is analogous to PCA-based variable selection techniques (Jolliffe 1972, Enki et al  2013) The main contributions of this paper can be summarized as follows •We propose P3C procedures for solving large-scale R&S problems  We establish a theoret- ical underpinning of the interaction between mean and correlation information and prove that',\n  'correlation-based clustering can achieve rate-optimal sample complexity reduction  Zhang and Peng: Parallel Correlation-Based Clustering And Conquer6 •We introduce a new selection policy, named P-OS, tailored for large-scale scenarios, along with a corresponding GBA sampling policy •We propose a parallelizable few-shot alternative clustering algorithm, denoted as AC+  This algorithm eliminates the need to estimate the entire correlation matrix and only requires a small submatrix, effectively addressing the challenges of large-scale clustering',\n  ' We introduce the proba- bility of correct clustering (PCC) to quantify the clustering quality 2  Problem Formulation LetP={1,2, ,p}denote the index set for all palternatives  We adopt a Frequentist frame- work, and the output of alternative i∈ Pis a random variable Xi  We assume that the population distribution of the random vector ( X1,X2,',\n  ',X p) is multivariate normal N(µ,Σp×p), where µ= (µ1,···,µp) is the mean vector and Σ p×pis the covariance matrix  Let xijdenote the jth sim- ulation observation of alternative i  The observation vectors ( x1j,x2j,···,xpj)∼N(µ,Σp×p) are independently and identically distributed  We assume',\n  'cov(xim,xjn) =\\uf8f1 \\uf8f4\\uf8f4\\uf8f2 \\uf8f4\\uf8f4\\uf8f30 m̸=n cov(Xi,Xj)m=n, where cov(X,Y) represents the covariance between random variables XandY  Let Nidenote the individual sample size allocated to alternative iand ¯xibe the sample average, then cov(¯xi,¯xj)= cov(Xi,Xj) max( Ni,Nj)',\n  ' The objective of R&S is to identify the true best alternative [1]= arg max i∈Pµi The correlation within the distribution arises from the influence of common latent factors, reflecting the similarity between alternatives  Suppose that the palternatives come from knon- overlapping clusters: G1, ,Gk(k≤p)with cardinality |Gj|=pj(Pk j=1pj=p) Gis a mapping from Pto{1,2,',\n  'to bypassing the traditional need for screening via pairwise comparison—a step previously deemed essential for high sample efficiency but problematic for parallelization  Additionally, we propose a parallel few-shot clustering algorithm tailored for large-scale problems Key words : simulation, ranking and selection, parallel computing 1  Introduction Ranking and selection (R&S) aims to identify the best design from a finite set of alternatives, through conducting simulation and learning about their performances  In recent years, the large-',\n  ',k}, where G(i)=jif alternative ibelongs to Gj  Π = ( G(1),G(2), ,G (p))∈Rp represents the true cluster partition of palternatives (unknown) and we need sampling to recover it  Alternatives within the same cluster are highly correlated (although their mean performances may vary considerably), whereas alternatives from different clusters are less correlated  We assume',\n  'that the Pearson correlation coefficient between any two alternatives belonging to the same cluster Zhang and Peng: Parallel Correlation-Based Clustering And Conquer7 exceeds that between any two alternatives from different clusters  Notably, the independent case is also included by setting p=k Assumption 1  rij> rmnfor all i,j,m,n ∈ Psuch that G(i) =G(j)andG(m)̸=G(n), where rij',\n  'denotes the correlation coefficient between XiandXjand0< rij<1 3  The Framework of Parallel Correlation-Based Clustering and Conquer Since the alternatives in the same cluster reflect common latent factors and attributes, one may select a representative alternative from each cluster to reduce complexity  For example, if the outputs of two alternatives satisfy X1=X2+ϵwhere ϵrepresents white noise, we only need to simulate one of them to gain knowledge about both alternatives',\n  ' The development of P3C procedures is inspired by this intuition 3 1  Clustering and Conquer As outlined in Procedure 1 and Figure 2, P3C starts with an initialization Stage 0  Then, in Stage 1, we continue sampling and then cluster alternatives based on correlations  The sample size can be determined in advance based on the required clustering accuracy  Additionally, the novel few-shot clustering algorithm AC+, detailed in Chapter 5, enables the effective parallelization of',\n  'both Stage 0 and Stage 1 in large-scale problems  Upon completion of Stage 1, the alternatives of the same cluster are sent to a single processor  During Stage 2, we continue sampling and then select the local best within each cluster  This stage exclusively involves the comparison within the cluster, without any information exchange between clusters  The sampling strategy here is flexible, accommodating various existing fully-sequential methods like KT and GSP, which typically involve screening, i e',\n  ', discarding clearly inferior alternatives early on  We also provide a screening-free version that employs a sequential GBA sampling algorithm with stopping rules, referred to as P3C- GBA (see Chapter 6)  In Stage 3, we simulate and compare these kselected local bests to determine the final winner  Regarding statistical validity, if each cluster achieves a precision of 1 −α1in Stage',\n  '2, and if Stage 3 achieves 1 −α2, the overall statistical guarantee will be 1 −(α1+α2)  In practice, we set α1to be much larger than α2since pis much larger than k  Zhang and Peng: Parallel Correlation-Based Clustering And Conquer8 Procedure 1 Parallel Correlation-based Clustering and Conquer (P3C) Stage 0 (Initialization)',\n  ' Simulate N0times for each alternative and estimate the correlation matrix Stage 1 (Alternative clustering)  Continue sampling  Perform clustering algorithm AC+to group all palternatives into kcorrelated clusters  After that, assign the alternatives of the same cluster to one processor Stage 2 (Selecting the representative local best)  Continue sampling (using KT, GSP or GBA) and calculate the P-OS within each cluster Stage 3 (Final comparison)',\n  ' R&S on the selected klocal P-OSs and find the global optimal alternative Figure 2 Parallel correlation-based clustering and conquer P3C can be viewed as a variant of the “divide and conquer” strategy (Ni et al  2017, Zhong and Hong 2022)  The first departure is that P3C performs correlation-based clustering and then systematically assigns alternatives from the same cluster to a single processor, rather than ran- domly assigning them',\n  ' We prove that correlation-based clustering can bring rate-optimal sample complexity reduction  The second departure is the adoption of the aforementioned sequential GBA algorithm (optional)  It can adapt to both fixed-precision and fixed-budget formulations  Moreover, it leverages correlation information and exhibits high sample efficiency in large-scale experiments  Zhang and Peng: Parallel Correlation-Based Clustering And Conquer9 3 2  Probabilistic Optimal Selection Policy',\n  'scale R&S problem, especially in the context of parallel computing environments, has emerged as an important research topic (Luo et al  2015, Ni et al  2017, Zhong and Hong 2022)  Here, 1arXiv:2402 02196v2  [stat ME]  13 Feb 2024 Zhang and Peng: Parallel Correlation-Based Clustering And Conquer2',\n  'The third departure from the original “divide and conquer” is adopting a new selection policy  The selection policy τis defined as a mapping from the information set to P(Peng et al  2016)  In traditional R&S procedures, the selection policy is specified as τm= arg maxi∈P¯xi(or maximizing the posterior mean in Bayesian framework), and correspondingly, the traditional PCS is defined as PCS trad≜P\\x00',\n  '¯x[1]>¯xj,j̸= [1]\\x01  To incorporate correlation information into the selection criterion, we borrow an idea from Bayesian R&S, which selects the one maximizing the posterior PCS rather than simply maximizing the posterior mean (Peng et al  2016, Russo 2020, Kim et al  2022)  First,',\n  'for any τ∈ P, we define the individual PCS of τas PCS (τ)≜P(¯xτ>¯xj,j̸=τ)  This is the natural generalization of traditional PCS as PCS trad= PCS ([1])  According to Hong et al  (2021), the statistical meaning of PCS (τ)is the probability of rejecting the null hypothesis that “ τisnotthe true best”, serving as a metric to evaluate the performance of τ',\n  ' Then the P-OS of Pis defined as the alternative maximizing PCS (τ): τ∗≜arg max τ∈PPCS(τ)  (1) This definition can be readily adapted to any cluster Gj⊆ P,j= 1,···,k  In practice, PCS (τ)and τ∗are calculated by plugging in parameters estimated with samples collected so far  Naturally,',\n  'the P-OS policy corresponds to a new statistical guarantee: PCS( τ∗), referred to as the the most optimistic probability of correct selection (moPCS)  The moPCS can be used as a relaxed probability guarantee because moPCS ≥PCS trad  In our large-scale AI experiments, moPCS can even be several tens of times higher than PCS trad The benefits and properties of the P-OS policy, extensively discussed in Chapter 4, can be sum- marized as follows',\n  ' First, the P-OS will converge to [1] as the sample size approaches infinity Second, with finite samples, the P-OS policy is proved to select a desirable representative local best within each cluster that not only has a large mean but also exhibits high correlations with other cluster members  This brings us back to the intuition behind P3C, i e , to reduce the problem Zhang and Peng: Parallel Correlation-Based Clustering And Conquer10 complexity by selecting a representative from each cluster',\n  ' Third, while the analysis and method- ologies in this paper remain valid for the traditional selection policy τmand performance metric PCS trad(by letting τ∗= [1]), the adoption of P-OS and moPCS is particularly advan',\n  '“large-scale” refers to a large number of alternatives, denoted by p  Most of the prominent existing large-scale R&S procedures are fully-sequential in nature, which are usually more sample-efficient due to the inclusion of screening steps based on pairwise comparisons  However, they encounter considerable hurdles in parallel implementation due to frequent information communication and synchronization issues (Luo et al  2015, Ni et al  2017)  Recent advancements have focused on',\n  'overcoming the limitations of the all pairwise comparison paradigm  For instance, Zhong and Hong (2022) propose a Knockout Tournament (KT) procedure to restrict comparisons to matches involving only two alternatives  Pei et al  (2022) compare each alternative against a common standard to avoid exhaustive pairwise comparisons  On the contrary, stage-wise methods, which predetermine the number of replications, are more naturally parallelizable  Nonetheless, a notable',\n  'drawback of these methods is their typically high requirement for sample sizes To break this dilemma, we propose parallel correlation-based clustering and conquer (P3C) pro- cedures, which leverage correlation information among alternatives for clustering to enhance sample efficiency  Existing R&S methods typically assume independence among alternatives and discard shared structural information (Eckman and Henderson 2022), with exceptions such as Fu et al (2007), Qu et al  (2015), Li et al',\n  ' (2022), Zhou et al  (2023)  However, none of these existing meth- ods that utilize shared information is tailored for large-scale problems, and they lack a rigorous mathematical analysis to elucidate and quantify the benefits derived from integrating similarity information, despite showing promising experimental results  Zhong and Hong (2022) point out that it is important to understand the growth rate of the required total sample size as pincreases',\n  'Along this line, we prove that correlation-based clustering techniques in P3C can achieve an O(p) reduction in sample complexity, which is theoretically the optimal reduction rate Screening via frequent pairwise comparison is often considered to be the cornerstone for achieving high sample efficiency in R&S (Hunter and Nelson 2017), despite its difficulty in parallelization  In this paper, valuable correlation information offers a new promising avenue: an experiment with 105'],\n 'uris': None,\n 'data': None}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.get()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T07:34:24.701767700Z",
     "start_time": "2024-10-09T07:34:24.684048100Z"
    }
   },
   "id": "783b94e2745743d4",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'ids': ['ids6'],\n 'embeddings': None,\n 'metadatas': [None],\n 'documents': ['overcoming the limitations of the all pairwise comparison paradigm  For instance, Zhong and Hong (2022) propose a Knockout Tournament (KT) procedure to restrict comparisons to matches involving only two alternatives  Pei et al  (2022) compare each alternative against a common standard to avoid exhaustive pairwise comparisons  On the contrary, stage-wise methods, which predetermine the number of replications, are more naturally parallelizable  Nonetheless, a notable'],\n 'uris': None,\n 'data': None}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.get(ids='ids6')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T14:36:50.023675Z",
     "start_time": "2024-10-08T14:36:49.995804Z"
    }
   },
   "id": "f93b8bd18b3197c1",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('01')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T02:43:51.329936200Z",
     "start_time": "2024-10-10T02:43:51.297289800Z"
    }
   },
   "id": "303f082105920790",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 a\n",
      "2 b\n",
      "3 c\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = ['a', 'b', 'c']\n",
    "for i, j in zip(a, b):\n",
    "    print(i, j)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T03:05:18.517488Z",
     "start_time": "2024-10-10T03:05:18.467065800Z"
    }
   },
   "id": "6c1a416fa9512a0f",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8fd15cd5c35dc6b8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
